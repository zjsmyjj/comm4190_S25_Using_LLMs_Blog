{
 "cells": [
  {
   "cell_type": "raw",
   "id": "defc79a9-5b23-43ee-94d2-d76f645d799c",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"How to use CoT prompting? Yu et al. (2023)\"\n",
    "description: \"Summarizing the key points of the paper: Yu, Z., He, L., Wu, Z., Dai, X., & Chen, J. (2023). Towards better chain-of-thought prompting strategies: A survey.\"\n",
    "author: \"Qijia Ye\"\n",
    "date: \"03/01/2024\"\n",
    "categories:\n",
    "  - LLMs\n",
    "  - Prompts\n",
    "  - CoT\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d76f8e7-219d-4f79-ab03-40fc48800d15",
   "metadata": {},
   "source": [
    "[Yu et al. (2023)](https://arxiv.org/abs/2310.04959) reviews the research on chain-of-though (CoT) prompting strategies and provides a systematic analysis on the factors that influence the performance of CoT prompting strateges. Here I summarize the key points from the paper that can help LLM users to use CoT prompting stratgies more effectively. The paper mainly discusses four factors that significantly affect the performance of CoT prompting, including **task types**, **prompt design**, **extension strategies**, and **models**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006e94b2-3ae5-4adc-95a6-6bf0532eb01f",
   "metadata": {},
   "source": [
    "# Task Type\n",
    "The paper categorizes LLM-related tasks into three: close domain reasoing, open domain reasoning, and code generation.\n",
    "### Close domain reasoning\n",
    "* Close domain reasoning includes necessary conditions and background knowledge in the problems. CoT prompting usually lead to high performance in close domain reasoing.\n",
    "### Opeon domain reasoning\n",
    "* Open domain reasoning does not include necessary conditions and background knowledge in the problems. The performance of CoT prompting highly depends on the the LLM knowledge quality. Using CoT prompting improperly may lower the performance.\n",
    "### Code generation\n",
    "* CoT prompting is suitable for code generation task because of the similar reasoning chain."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "faa2a008-a457-4c92-b4ca-fa59a6463c06",
   "metadata": {},
   "source": [
    "# Prompt Design\n",
    "CoT prompts consist of two essential components: *demonstrations* and *text instructions*. Demonstrations refer to step-wise reasoning exemplars (i.e., few-shot prompts). Text instructions refer to texts that guide a progressive reasoning (e.g., \"Let's think step by step.\").\n",
    "### Demonstrations\n",
    "* When design a CoT prompt (few-shot prompt), an LLM user needs to consider the complexity of demonstration problem since a more difficult problem may need longer reasoning steps.\n",
    "\n",
    "* The user also needs to consider the relevance and diversity of demonstrations. It is a trade-off process (e.g., more relevant demonstrations often lead to lower diversity of demonstrations). \"Too monotonous demonstrations may lead to a less robust model performance while involving too many irrelevant demonstrations may inject more noise.\"\n",
    "\n",
    "* Bridging objects and language templates can jointly reduce the ambiguity of demonstration rationales.\n",
    "\n",
    "* \"A valid CoT rationale does promote a better prompting performance while an effective CoT prompt does not necessarily demand a totally valid rationale.\"\n",
    "\n",
    "* Demonstrations number influences the performance of CoT prompts. Model performance will be enhanced if demonstrations number increases from 0 to 2, but the improvement remains slowly if the number continually increases. The order of demonstrations may influence the performance, but there is no conclusion yet.\n",
    "### Text instructions\n",
    "* Text instructions improve LLMsâ€™ performance regardless of using zero-shot CoT or few-shot CoT prompting strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da5b37e-8a35-4ec0-9171-ba158b26e27b",
   "metadata": {},
   "source": [
    "# Extension Strategies\n",
    "The paper also introduces four extenstion strategies that assist in CoT prompting, including **ensemble**, **sub-problem divison**, **external assistance**, and **rationalization**.\n",
    "### Ensemble\n",
    "* Ensemble learning is a technique that combines multiple models, multiple prompts (i.e., prompts ensemble), or multiple generated responses (i.e., predicitons ensemble) to improve the performance of LLMs. It can help reduce errors but may also inject noise to a confident prediction.\n",
    "### Sub-problem division\n",
    "* When confronting a problem needs to be recursively inferred or harder than demonstrations, dividing a problem into several sub-problems may improve the performance.\n",
    "### External assistance\n",
    "* Incorporating external assistance can be helpful in the task. The external assistance includes knowledge injection, external tools, code intpreters, and other LLMs.\n",
    "### Rationalization\n",
    "* Rationalizing the reasoning process may correct the mistake that an LLM made. A simple way is to tell the LLM the correct answer and ask it rethink the reasoning process (i.e., self-learning)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff7c39f-e2cb-4f25-a48c-8d8ca7661c32",
   "metadata": {},
   "source": [
    "# Models\n",
    "### Model size\n",
    "* When an LLM is relatively small in terms of its size, CoT prompting does not influence the performance too much.\n",
    "* When an LLM is large (over 10 billion parameters), CoT prompting can improve the performance a lot.\n",
    "### Train corpus\n",
    "* Training corpus influences the effect of CoT prompting. When the train corpus includes codes or CoT-like examples, CoT prompting are more effective."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
