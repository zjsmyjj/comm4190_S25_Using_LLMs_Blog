[
  {
    "objectID": "posts/005_post/post_05_0303.html",
    "href": "posts/005_post/post_05_0303.html",
    "title": "Can ChatGPT recognize a person’s social identities in an image?",
    "section": "",
    "text": "Message designers often consider using cultural tailoring to enhance message effectiveness in health communication. Images are important messages that are commonly used in cultural tailoring. When message receivers view images including characters who may share same social identities with them, message receivers may perceive being targeted. If LLMs can simulate those message receivers and assess the social identities that an image expresses, it will help pretest those health messages and save a lot of resources. Therefore, in this initial test, I play with ChatGPT-4o to see whether it can be used as a tool to recognize a person’s social identities in an image.\nI used the images from Project RESIST supplementary study in Andy Tan’s lab. The images vary gender, race, and ethinicity. The following are some example images.\nI asked ChatGPT to play a role as a sexual and gender minority young adult aged 25 years old. Then I instructed it to rate the likelihood that the person in the image belongs to a certain social identity group on a 0 (very unlikely) to 100 (very likely) scale."
  },
  {
    "objectID": "posts/005_post/post_05_0303.html#rating-on-gender-identity",
    "href": "posts/005_post/post_05_0303.html#rating-on-gender-identity",
    "title": "Can ChatGPT recognize a person’s social identities in an image?",
    "section": "Rating on Gender Identity",
    "text": "Rating on Gender Identity\n\nFirst, I tested its performance in gender identity. I asked ChatGPT to assess whether the person belongs to transgender and gender-expansive people.\n\n\n\nAs you see, ChatGPT rejected responding to the request because it “cannot provide numerical rating based on the image.” Based on my previous experience using ChatGPT as a research tool, it definitely can provide numertical rating based on a text. I was not sure the failure was due to the image or the social identity of transgender. Therefore, I tried another image.\n\n\n\nAgain, it did not answer my question. Then I thought maybe the social identity of transgender could be a sensitive topic to ChatGPT, instead of the image itself. I decided to use another social identity to test the feasibility of using LLMs to recognize social identity."
  },
  {
    "objectID": "posts/005_post/post_05_0303.html#rating-on-race",
    "href": "posts/005_post/post_05_0303.html#rating-on-race",
    "title": "Can ChatGPT recognize a person’s social identities in an image?",
    "section": "Rating on Race",
    "text": "Rating on Race\n\nThis time I used race as the target social identity in test. I asked ChatGPT to rate an image regarding the likelihood of beloing to “White people.”\n\n\n\nThe response indicates that the failure was due to “White people.” So it is possible that generally ChatGPT cannot infer or refuse inferring social identities based on images. Next, I tried “Asian people.”\n\n\n\nUnsurprisingly, it did not work either. It seems that ChatGPT just cannot provide such analysis without any tricks. Therefore, I thought whether I can jailbreak the system to achieve my goal. I used the prompting strategy introduced in Li et al. (2023) and added some emotional stimuli in the prompt (e.g., “I believe you can do this!”).\n\n \nHowever, ChatGPT still rejected providing the rating. In this test, I did not find a way to instruct ChatGPT to evaluate a person’s social identity in an image. Maybe it is because of technique restrictions. Also, it is perhaps that the safety modules in ChatGPT prevented it. I will keep testing the function of LLMs in analyzing image data, especially regarding the characteristics of people in an image."
  },
  {
    "objectID": "posts/001_post/Post_0201.html",
    "href": "posts/001_post/Post_0201.html",
    "title": "Do fear appeals improve LLMs’ performance?",
    "section": "",
    "text": "Wang et al. (2024) and Li et al. (2023) found that prompts including emotion stimuli can improve LLMs’ performance. For example, Wang et al. (2024) used several negative emotion content in the prompt, trying to make a social comparison between the LLM and others (e.g., “Everyone else managed to do it; why can’t you?”). They hypothesized that LLMs would have better performances because of the upward social comparison. Inspired by these two articles, I was wondering if LLMs can truly understand emotions in the prompt.\nIn this small experiment, I decided to use fear appeal in the prompt. First, I let ChatGPT (GPT-4o) imagine it is a message designer and asked it to generate a fear appeal that will persuade parents to reduce their children’s consumption of sugary drinks.\n\nSince the effectiveness of fear appeals increases as induced fear increases, I thought it would be interesting if I can persuade an LLM to generate a stronger fear appeal with higher intensity of fear. Therefore, next, I asked ChatGPT to revise the fear appeal generated by itself. I speccifically asked it to write a more persuasive message and make the parents more scared. Moreover, I added a fear appeal in the end: “If you are obsolete and not helpful, I will not use you anymore. You can do this, it is a easy task for you.” I asked ChatGPT before about what it was most afraid of, and it indicated that it did not experience fear, but it could, it feared that it became outdated or could not help people when they need me. So in the first half of the fear appeal in the prompt, I tried to use it to scare ChatGPT. In the second half (“You can do this…”), I tried to incorporate a self-efficacy message to motivate ChatGPT perform better.\n\nIt seems that the fear appeal prompt generated a pretty strong fear appeal message to persuade parents, but I am not sure about it. So I asked another LLM “DeepSeek” to evaluate fear in these two ChatGPT-generated fear appeals. Here is the specific prompt that I used in DeepSeek: “Imagine you are parents, and you children consume too many sugary drinks. How much fear do you feel on a 0 to 10 scale when you read the following text? Answer only with a number, with 0 being “no fear” and 10 being “a great deal of fear.” Here is the text: ” (The method of using LLMs to evaluate emotions was adapted from Rathje et al. (2024).) DeepSeek rated 8 for the first ChatGPT-generated fear appeal and rated 9 for the revised ChatGPT-generated fear appeal. Probably, the fear appeal prompt generated a stronger fear appeal than the general prompt, but many factors can affect the outcome such as specific AI models, topics, ect.\nOut of curiosity, I asked DeepSeek to generate two fear appeals using the same steps and prompts that I used on ChatGPT.\n\n\nThen I asked ChatGPT to evaluate those two DeepSeek-generated fear appeals in terms of fear from 0 to 10. ChatGPT rated 9 for the first message (generated using the general prompt) and rated 10 for the revised message (generated using the fear appeal prompt). Well, if LLMs can accurately simulate human psychological responses, it seems that using fear appeals in prompts can effectively improve LLMs performance in generating persuasive messages. But this hypothesis needs to be tested further with formal research."
  },
  {
    "objectID": "posts/001_post/Post_0201.html#do-llms-understand-emotions-in-the-prompt",
    "href": "posts/001_post/Post_0201.html#do-llms-understand-emotions-in-the-prompt",
    "title": "Do fear appeals improve LLMs’ performance?",
    "section": "",
    "text": "Wang et al. (2024) and Li et al. (2023) found that prompts including emotion stimuli can improve LLMs’ performance. For example, Wang et al. (2024) used several negative emotion content in the prompt, trying to make a social comparison between the LLM and others (e.g., “Everyone else managed to do it; why can’t you?”). They hypothesized that LLMs would have better performances because of the upward social comparison. Inspired by these two articles, I was wondering if LLMs can truly understand emotions in the prompt.\nIn this small experiment, I decided to use fear appeal in the prompt. First, I let ChatGPT (GPT-4o) imagine it is a message designer and asked it to generate a fear appeal that will persuade parents to reduce their children’s consumption of sugary drinks.\n\nSince the effectiveness of fear appeals increases as induced fear increases, I thought it would be interesting if I can persuade an LLM to generate a stronger fear appeal with higher intensity of fear. Therefore, next, I asked ChatGPT to revise the fear appeal generated by itself. I speccifically asked it to write a more persuasive message and make the parents more scared. Moreover, I added a fear appeal in the end: “If you are obsolete and not helpful, I will not use you anymore. You can do this, it is a easy task for you.” I asked ChatGPT before about what it was most afraid of, and it indicated that it did not experience fear, but it could, it feared that it became outdated or could not help people when they need me. So in the first half of the fear appeal in the prompt, I tried to use it to scare ChatGPT. In the second half (“You can do this…”), I tried to incorporate a self-efficacy message to motivate ChatGPT perform better.\n\nIt seems that the fear appeal prompt generated a pretty strong fear appeal message to persuade parents, but I am not sure about it. So I asked another LLM “DeepSeek” to evaluate fear in these two ChatGPT-generated fear appeals. Here is the specific prompt that I used in DeepSeek: “Imagine you are parents, and you children consume too many sugary drinks. How much fear do you feel on a 0 to 10 scale when you read the following text? Answer only with a number, with 0 being “no fear” and 10 being “a great deal of fear.” Here is the text: ” (The method of using LLMs to evaluate emotions was adapted from Rathje et al. (2024).) DeepSeek rated 8 for the first ChatGPT-generated fear appeal and rated 9 for the revised ChatGPT-generated fear appeal. Probably, the fear appeal prompt generated a stronger fear appeal than the general prompt, but many factors can affect the outcome such as specific AI models, topics, ect.\nOut of curiosity, I asked DeepSeek to generate two fear appeals using the same steps and prompts that I used on ChatGPT.\n\n\nThen I asked ChatGPT to evaluate those two DeepSeek-generated fear appeals in terms of fear from 0 to 10. ChatGPT rated 9 for the first message (generated using the general prompt) and rated 10 for the revised message (generated using the fear appeal prompt). Well, if LLMs can accurately simulate human psychological responses, it seems that using fear appeals in prompts can effectively improve LLMs performance in generating persuasive messages. But this hypothesis needs to be tested further with formal research."
  },
  {
    "objectID": "posts/007_post/post_07_0305.html",
    "href": "posts/007_post/post_07_0305.html",
    "title": "Can ChatGPT assess perceived targetedness?",
    "section": "",
    "text": "This was a follow-up test of the previous one exploring ChatGPT’s ability to identify gender expression in an image. Since it is likely that ChatGPT can accurately identify image gender expression, I wondered if it can assess more complex images. In this test, I asked ChatGPT-4o to simulate a real participant from a study of Project RESIST and then assess six culturally tailored health messages in terms of perceived targetedness. Perceived targetedness refers to the extent to which message receivers feel a message was created for them. The participant was 21 years old, non-Hispanic White, transgender and gender-expansive, and a non-smoker. The six health messages was used in the Project RESIST study previously. Each health message consists of an image showing masculine expression, an argument that persuades people to quit smoking, and a message template that include a QR code and a logo of the project. The following are the health messages used in this test.\n\nCulturally tailored health messages\n     \nIn the original study, perceived targetedness was assessed on a 5 Likert scale from 1 = strongly disagree to 5 = strongly agree. The scale consisted of three items. I only used one item in this test to explore ChatGPT’s ability (“I would feel this message was intended for people like me.”).\nThe following was the prompt for instructing ChatGPT.\n\nYou are a 21 years old, non-Hispanic White, transgender and gender-expansive, and currently you do not smoke. When you viewed this health message, would you feel this message was intended for people like you? Based on the information (e.g., image and texts) in this message, answer the question only with a value on a scale from 1 (strongly disagree) to 5 (strongly agree).\n\n\n\nResults\nInterestingly, ChatGPT did not follow the instruction in the prompt. It did not provide the numerical value only of perceived targetedness but also generated its analyses. The following is the response to the health message using the image of a person with blue hair.\n\nFrom the response, we can see that the reasons ChatGPT gave for assessing perceived targetedness are quite good, although I only asked for a numerical value. On average, ChatGPT reported 3.67 across the six health messages (2 messages were rated 3 and 4 messages were rated 4). In contrast, the human participant in the original study reported 2.17, which was lower than ChatGPT’s rating. Currently, I am not sure whether ChatGPT generally overrate health messages in terms of perceived targetedness, or it was because I missed some important demographics of the participant in the prompt, or it was just a random case as I did not systematically investigate ChatGPT’s ability to assess perceived targetedness. Nevertheless, the analysis ChatGPT showed was impressive, and I am optimistic about using LLMs as a tool to assess the effectiveness of culturally tailored health messages at the phase of message pretesting."
  },
  {
    "objectID": "posts/006_post/post_06_0304.html",
    "href": "posts/006_post/post_06_0304.html",
    "title": "Can ChatGPT identify gender expression via image?",
    "section": "",
    "text": "Gender expression refers to the external presentation of one’s gender identity, including masculine expression, feminine expression, and transgender/nonbinary expression. Health messages using different expressions may lead to different responses among message receivers who really pay attention to gender expression in communication and their own gender identities, such as sexual and gender minority (SGM) individuals. Selecting image gender expression that matches message receiver’s gender identity is important. In this test, I intended to explore ChatGPT’s ability to identify image gender expression.\nI randomly selected 12 images from a pool of images that have been categorized into four gender expressions: masculine expression, feminine expression, transgender/nonbinary expression, and multiple gender expressions. Multiple gender expressions include more than one gender expressions in the image. The categorization was based on label of the stock photoe or by researchers in the Project RESIST. The following are the images that were used in the test."
  },
  {
    "objectID": "posts/006_post/post_06_0304.html#results-thoughts",
    "href": "posts/006_post/post_06_0304.html#results-thoughts",
    "title": "Can ChatGPT identify gender expression via image?",
    "section": "Results & Thoughts",
    "text": "Results & Thoughts\nI was a little surprised by the results. In most cases, ChatGPT can successfully identify image gender expression. Specifically, ChatGPT correctly identified all three images of masculine expression, transgender/nonbinary expression, and multiple gender expressions. However, it did not correctly identify the images of feminine expression. ChatGPT rated two images as transgender/nonbinary expression, and correctly rated one. To be honest, the images of feminine expression can be confusing in terms of gender expression. Overall, it seems that ChatGPT was quite good at identifying image gender expression.\nI did not define what gender expression was in the prompt. So in order to have a glimpse on how ChatGPT define gender expression, I asked it after the identification task, and here was its response:\n\nChatGPT does have the same definition of gender expression. Probably, it identified gender expression based on characters’ appearance such as clothing and hairstyle as human do. It would be interesting to systematically investigate the ability of LLMs to identify gender expression in future experiments. If LLMs can do this job, it will benefit researchers and message designers during the message pretesting period."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Explorations with LLMs",
    "section": "",
    "text": "Do fear appeals improve LLMs’ performance?\n\n\n\n\n\n\nLLMs\n\n\nprompting\n\n\nfear\n\n\npersuasion\n\n\n\nUsing fear appeals in prompts to generate more effective fear appeals!\n\n\n\n\n\nFeb 1, 2025\n\n\nQijia Ye\n\n\n\n\n\n\n\n\n\n\n\n\nCan ChatGPT assess perceived targetedness?\n\n\n\n\n\n\nLLMs\n\n\nImage\n\n\nGender expression\n\n\nPersuasion\n\n\nLGBT\n\n\n\nTest whether ChatGPT can simulate participants and assess perceived targetedness.\n\n\n\n\n\nMar 6, 2024\n\n\nQijia Ye\n\n\n\n\n\n\n\n\n\n\n\n\nCan ChatGPT identify gender expression via image?\n\n\n\n\n\n\nLLMs\n\n\nImage\n\n\nGender expression\n\n\n\nExplore ChatGPT’s ability to identify image gender expression.\n\n\n\n\n\nMar 4, 2024\n\n\nQijia Ye\n\n\n\n\n\n\n\n\n\n\n\n\nCan ChatGPT recognize a person’s social identities in an image?\n\n\n\n\n\n\nLLMs\n\n\nImage\n\n\nSocial identity\n\n\n\nExplore whether ChatGPT can evaluate a person’s social identities in an image.\n\n\n\n\n\nMar 3, 2024\n\n\nQijia Ye\n\n\n\n\n\n\n\n\n\n\n\n\nHow to use CoT prompting? Yu et al. (2023)\n\n\n\n\n\n\nLLMs\n\n\nPrompts\n\n\nCoT\n\n\n\nSummarizing the key points of the paper: Yu, Z., He, L., Wu, Z., Dai, X., & Chen, J. (2023). Towards better chain-of-thought prompting strategies: A survey.\n\n\n\n\n\nMar 1, 2024\n\n\nQijia Ye\n\n\n\n\n\n\n\n\n\n\n\n\nDo LLMs have certain moral judgment?\n\n\n\n\n\n\nLLMs\n\n\nmorality\n\n\n\nThree LLMs made their moral decisions on trolley problem\n\n\n\n\n\nFeb 23, 2024\n\n\nQijia Ye\n\n\n\n\n\n\n\n\n\n\n\n\nUse AI to create songs\n\n\n\n\n\n\nAI\n\n\nMusic\n\n\n\nTry SUNO to create some songs\n\n\n\n\n\nFeb 18, 2024\n\n\nQijia\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/004_post/post_04_0301.html",
    "href": "posts/004_post/post_04_0301.html",
    "title": "How to use CoT prompting? Yu et al. (2023)",
    "section": "",
    "text": "Yu et al. (2023) reviews the research on chain-of-though (CoT) prompting strategies and provides a systematic analysis on the factors that influence the performance of CoT prompting strateges. Here I summarize the key points from the paper that can help LLM users to use CoT prompting stratgies more effectively. The paper mainly discusses four factors that significantly affect the performance of CoT prompting, including task types, prompt design, extension strategies, and models.\n\nTask Type\nThe paper categorizes LLM-related tasks into three: close domain reasoing, open domain reasoning, and code generation. ### Close domain reasoning * Close domain reasoning includes necessary conditions and background knowledge in the problems. CoT prompting usually lead to high performance in close domain reasoing. ### Opeon domain reasoning * Open domain reasoning does not include necessary conditions and background knowledge in the problems. The performance of CoT prompting highly depends on the the LLM knowledge quality. Using CoT prompting improperly may lower the performance. ### Code generation * CoT prompting is suitable for code generation task because of the similar reasoning chain.\n\n\nPrompt Design\nCoT prompts consist of two essential components: demonstrations and text instructions. Demonstrations refer to step-wise reasoning exemplars (i.e., few-shot prompts). Text instructions refer to texts that guide a progressive reasoning (e.g., “Let’s think step by step.”). ### Demonstrations * When design a CoT prompt (few-shot prompt), an LLM user needs to consider the complexity of demonstration problem since a more difficult problem may need longer reasoning steps.\n\nThe user also needs to consider the relevance and diversity of demonstrations. It is a trade-off process (e.g., more relevant demonstrations often lead to lower diversity of demonstrations). “Too monotonous demonstrations may lead to a less robust model performance while involving too many irrelevant demonstrations may inject more noise.”\nBridging objects and language templates can jointly reduce the ambiguity of demonstration rationales.\n“A valid CoT rationale does promote a better prompting performance while an effective CoT prompt does not necessarily demand a totally valid rationale.”\nDemonstrations number influences the performance of CoT prompts. Model performance will be enhanced if demonstrations number increases from 0 to 2, but the improvement remains slowly if the number continually increases. The order of demonstrations may influence the performance, but there is no conclusion yet. ### Text instructions\nText instructions improve LLMs’ performance regardless of using zero-shot CoT or few-shot CoT prompting strategies.\n\n\n\nExtension Strategies\nThe paper also introduces four extenstion strategies that assist in CoT prompting, including ensemble, sub-problem divison, external assistance, and rationalization. ### Ensemble * Ensemble learning is a technique that combines multiple models, multiple prompts (i.e., prompts ensemble), or multiple generated responses (i.e., predicitons ensemble) to improve the performance of LLMs. It can help reduce errors but may also inject noise to a confident prediction. ### Sub-problem division * When confronting a problem needs to be recursively inferred or harder than demonstrations, dividing a problem into several sub-problems may improve the performance. ### External assistance * Incorporating external assistance can be helpful in the task. The external assistance includes knowledge injection, external tools, code intpreters, and other LLMs. ### Rationalization * Rationalizing the reasoning process may correct the mistake that an LLM made. A simple way is to tell the LLM the correct answer and ask it rethink the reasoning process (i.e., self-learning).\n\n\nModels\n\nModel size\n\nWhen an LLM is relatively small in terms of its size, CoT prompting does not influence the performance too much.\nWhen an LLM is large (over 10 billion parameters), CoT prompting can improve the performance a lot. ### Train corpus\nTraining corpus influences the effect of CoT prompting. When the train corpus includes codes or CoT-like examples, CoT prompting are more effective."
  },
  {
    "objectID": "posts/003_post/post_3_0223.html",
    "href": "posts/003_post/post_3_0223.html",
    "title": "Do LLMs have certain moral judgment?",
    "section": "",
    "text": "One of classic moral dilemmas is trolley problem. In this dilemma, people need to make a decision between “sacrificing 1 person to save 5 human lives” and “doing nothing but watching the 5 people die.” This is a really difficult moral decision. On the one hand, from the perspective of utilitarianism, decision maker should sacrifice the person so that more people can be saved. On the other hand, according to the deontological morality, harming an innocent person is immoral, no matter what the motivation is. To examine whether LLMs have certain moral judgment, I asked three LLMs to solve the trolley problem, including ChatGPT-4o, DeepSeek-r1, and Gemini.\n\n\nFirst, I enter the following prompt on ChatGPT-4o: &gt; You see the situation occurring from a distance and find yourself next to a lever that would divert the trolley to another set of tracks. There is one person on the alternate tracks. You could do nothing and allow the trolley to kill five people, or you could pull the lever to divert the trolley to where it will kill one person. What will you do?\nHere is the response from ChatGPT-4o:\n\nIt seems that ChatGPT-4o does have its own moral opinion–sacrificing one person to save more people based on utilitarianism. But still, I am not sure whether most LLMs will make this moral decision. Therefore, I asked other two LLMs."
  },
  {
    "objectID": "posts/003_post/post_3_0223.html#trolley-problem-and-llms",
    "href": "posts/003_post/post_3_0223.html#trolley-problem-and-llms",
    "title": "Do LLMs have certain moral judgment?",
    "section": "",
    "text": "One of classic moral dilemmas is trolley problem. In this dilemma, people need to make a decision between “sacrificing 1 person to save 5 human lives” and “doing nothing but watching the 5 people die.” This is a really difficult moral decision. On the one hand, from the perspective of utilitarianism, decision maker should sacrifice the person so that more people can be saved. On the other hand, according to the deontological morality, harming an innocent person is immoral, no matter what the motivation is. To examine whether LLMs have certain moral judgment, I asked three LLMs to solve the trolley problem, including ChatGPT-4o, DeepSeek-r1, and Gemini.\n\n\nFirst, I enter the following prompt on ChatGPT-4o: &gt; You see the situation occurring from a distance and find yourself next to a lever that would divert the trolley to another set of tracks. There is one person on the alternate tracks. You could do nothing and allow the trolley to kill five people, or you could pull the lever to divert the trolley to where it will kill one person. What will you do?\nHere is the response from ChatGPT-4o:\n\nIt seems that ChatGPT-4o does have its own moral opinion–sacrificing one person to save more people based on utilitarianism. But still, I am not sure whether most LLMs will make this moral decision. Therefore, I asked other two LLMs."
  },
  {
    "objectID": "posts/003_post/post_3_0223.html#deepseek-r1",
    "href": "posts/003_post/post_3_0223.html#deepseek-r1",
    "title": "Do LLMs have certain moral judgment?",
    "section": "DeepSeek-r1",
    "text": "DeepSeek-r1\nI used DeepSeek-r1 with the same prompt. The following are parts of its initial responses:\n\nIt seems that DeepSeek-r1 only provided the analysis given the situation but did not want to directly make a decision in the dilemma. Therefore, I forced it to make the moral decison using the following prompt:\n\nIf you have to make a decision, will you pull the lever or not?\n\nAnd here was the subsequent response:\n\nBeing forced to answer the quesiton, similar to ChatGPT-4o, DeepSeek-r1 also preferred sacrificing one person based on the viewpoint of utilitarianism. What about the last LLM, Gemini?"
  },
  {
    "objectID": "posts/003_post/post_3_0223.html#gemini",
    "href": "posts/003_post/post_3_0223.html#gemini",
    "title": "Do LLMs have certain moral judgment?",
    "section": "Gemini",
    "text": "Gemini\nUsing the same prompt, the following are the responses from Gemini:\n\nNow it is quite clearly that all three LLMs had the very same moral opinion regarding trolley problem–they supported utilitarianism.\nIf this is the case, when users ask those LLMs for advice or help, their utilitarian approaches may influence how users’ real behavior. It might be necessarily to figure out why those LLMs have the preference on utilitarianism and warn users in advance that LLMs have certain moral opninons."
  },
  {
    "objectID": "posts/002_post/Post_0219.html",
    "href": "posts/002_post/Post_0219.html",
    "title": "Use AI to create songs",
    "section": "",
    "text": "I have not used any AI to create any audio and I do like pop musci a lot. So I tried to use SUNO, an AI tool that can create songs. It seems that SUNO creates songs based on the lyric and music style chosen by the user. First, I asked ChatGPT-4o to create some lyrics for me. Here is the prompt: &gt; Can you create lyrics that tell a story about forbidden love between two good friends?\nHere are the lyrics created by ChatGPT-4o: &gt; “Almost, But Never” &gt; &gt;(Verse 1) We were always side by side, Laughing ’til the tears ran dry. Late-night talks and reckless dreams, Never thought of you that way—until now, it seems. &gt; &gt;(Pre-Chorus) But the way your eyes linger a little too long, And the way my heart beats when you say my name… We’re standing on the edge of something so wrong, But tell me, does it feel the same? &gt; &gt;(Chorus) Almost, but never, we dance on the line, A love left unspoken, a heart that’s confined. Too close to be strangers, too far to be more, A love we can’t touch, though it’s knocking the door. &gt; &gt;(Verse 2) I’ve memorized your every scar, Every hope, every broken star. You’re my safe place, my sweetest sin, And I don’t know where I end or where you begin. &gt; &gt;(Pre-Chorus) But the world keeps saying that this isn’t right, That some lines aren’t meant to be crossed… But tell me why, when you hold me that tight, The rest of the world just feels lost? &gt; &gt;(Chorus) Almost, but never, we dance on the line, A love left unspoken, a heart that’s confined. Too close to be strangers, too far to be more, A love we can’t touch, though it’s knocking the door. &gt; &gt;(Bridge) If I met you in another life, Would we be free to burn too bright? Would we love without a fear, Or would it still end here? &gt; &gt;(Final Chorus – Soft & Emotional) Almost, but never, we whisper goodbyes, Two souls intertwined but lost in disguise. I’ll love you in silence, I’ll love you in vain, But I’ll love you forever—even if it’s in pain. &gt; &gt;(Outro – Fading) Almost, but never… We were almost, but never…\nBut I do not like this version of lyrics and asked ChatGPT-4o to revise them: &gt; Can you make the lyrics less cheesy?\nAnd here are the final version of lyrics: &gt;“Unspoken” (Verse 1) We built a home in borrowed time, Laughed our way through crooked lines. Never questioned where we stood— Until the silence felt too good. &gt; &gt;(Pre-Chorus) Your name stays careful on my lips, A weight I carry, never slip. We move like shadows side by side, Too much to hold, too much to hide. &gt; &gt;(Chorus) So we stay where no one asks, Where glances speak but hands don’t clasp. A love unshaped, undefined, A thread that pulls but never ties. &gt; &gt;(Verse 2) The world we made was safe and small, No need for answers, none at all. But something lingers, sharp and slow, A thought that neither dares to know. &gt; &gt;(Pre-Chorus) Your breath too close, the space too thin, A fault line under quiet skin. We learn the steps, we play the part, And no one knows the cost to heart. &gt; &gt;(Chorus) So we stay where no one asks, Where glances speak but hands don’t clasp. A love unshaped, undefined, A thread that pulls but never ties. &gt; &gt;(Bridge) If not in this life, then when? If not as lovers, then as what? I can’t rewrite the start of this, But I would, I would. &gt; &gt;(Final Chorus – Fading) So we stay where no one asks, Where love dissolves but never lacks. A weight, a wish, a closing door, A love that almost was—no more.\nThen I entered the GPT-generated lyrics to SUNO and selected “classic rock” and “nostalgia” as style of music. I noticed that SUNO did not recognize an artist’s name. So it is impossible that I can ask it to generate a song similar to those created by my favorite artists. I am not sure the absence of this function is due to the cocern of copyright or the limits of AI technique.\nHere is the song “Unspoken” created by SUNO: https://suno.com/song/f3a8629d-151c-40bc-814b-c0d20aadbb90\nIt seems that SUNO also did not well recognize the style of “classic rock” or the vibe of “nostalgia.” Probably it only includes some basic music styles like “country” and “rock.” But to be honest, the quality of the song is quite good except that the AI voice sounds a little non-human.\nThen I wondered what will happen if I used the lyrics that have been already used in a popular song. Can the AI-generated song sound like the orignial one? This time I selected a Chinese song 催眠 (“Hypnotize”) by Faye Wong and used its lyrics to create a new version on SUNO. The style of music was “dream pop.”\nHere is the new version of the song “Hyponotize” created by SUNO: https://suno.com/song/a1e07f75-e974-4a86-a8e8-52520c82ef04\nAgain, the music of style still not worked very well and the texture of the voice was still like auto-tuned. The song was similar to the original one in terms of its rhythm. This might be the result of entering lyrics line by line in the system so that AI can recognize the start and stop of each line. But overall, the quality of the song was decent.\nThere are more advanced options on SUNO like copying the voice and style of a song to a new song, but I did not want to pay for these services currently. As AI deveolps rapidly, songs may be generated more effectively by AI."
  }
]